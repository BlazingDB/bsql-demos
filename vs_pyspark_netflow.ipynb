{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d0hJ4z8rBOFC"
   },
   "source": [
    "# BlazingSQL vs. Apache Spark \n",
    "\n",
    "Below we have one of our popular workloads running with [BlazingSQL](https://blazingsql.com/) + [RAPIDS AI](https://rapids.ai) and then running the entire ETL phase again, only this time with Apache Spark + PySpark.\n",
    "\n",
    "In this notebook, we will cover: \n",
    "- How to set up BlazingSQL and the RAPIDS AI suite in Google Colab.\n",
    "- How to read and query csv files with cuDF and BlazingSQL.\n",
    "- How BlazingSQL compares against Apache Spark (analyzing over 20M records).\n",
    "\n",
    "![Impression](https://www.google-analytics.com/collect?v=1&tid=UA-39814657-5&cid=555&t=event&ec=guides&ea=bsql_vs_spark&dt=bsql_vs_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJyD4oSbugE0"
   },
   "source": [
    "## Setup\n",
    "### Environment Sanity Check \n",
    "\n",
    "RAPIDS packages (BlazingSQL included) require Pascal+ architecture to run. For Colab, this translates to a T4 GPU instance. \n",
    "\n",
    "The cell below will let you know what type of GPU you've been allocated, and how to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "QzVzojZ7tc9a",
    "outputId": "1c412c49-59fd-482b-83dc-1764af8fda12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woo! You got the right kind of GPU!\n"
     ]
    }
   ],
   "source": [
    "# tag specs\n",
    "colab_smi = !nvidia-smi\n",
    "\n",
    "# focus GPU type\n",
    "try:\n",
    "    my_gpu = ' '.join(colab_smi[7].split()[2:4])\n",
    "# not on gpu acceleration \n",
    "except:\n",
    "    raise Exception(\"\\nPlease make sure you've configured Colab to request a GPU instance type.\\n\\n\"\n",
    "                    \"At top of Colab, try: Runtime -> Change runtime type -> Hardware accelerator -> GPU -> Save\\n\")\n",
    "\n",
    "# not allocated compatable GPU\n",
    "if (my_gpu != b'Tesla T4') and (my_gpu != 'Tesla P100-PCIE...') and (my_gpu != 'GeForce GTX'):\n",
    "    # allocated K80\n",
    "    if my_gpu == 'Tesla K80':\n",
    "        raise Exception(\"\\nYou've been allocated a K80 instance\\n\\n\"\n",
    "                    \"Unfortunately, this demo requires a T4 instance\\n\\n\"\n",
    "                    \"At top of Colab, try: Runtime -> Reset all runtimes...\\n\")\n",
    "    else:\n",
    "        raise Exception(f\"\\nYou've achieved wizardy.\\nyour GPU is {my_gpu}\\nPlease inform info@blazingsql.com\")\n",
    "\n",
    "# allocated compatable GPU\n",
    "else:\n",
    "    print('Woo! You got the right kind of GPU!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "btG1BbSA1nLu",
    "outputId": "2bce70f9-8d92-4a16-baa2-2b190d785142"
   },
   "source": [
    "## Installs \n",
    "\n",
    "Below you will find three code blocks:\n",
    "1. The first installs miniconda.\n",
    "2. The second installs RAPIDS AI and sets up the system environment. \n",
    "3. The third installs BlazingSQL.\n",
    "\n",
    "### Miniconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqQ8lqL8vb-8"
   },
   "outputs": [],
   "source": [
    "# # intall miniconda\n",
    "# !wget -c https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
    "# !chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n",
    "# !bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAPIDS AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install RAPIDS packages\n",
    "# !conda install -q -y --prefix /usr/local -c nvidia -c rapidsai \\\n",
    "#   -c numba -c conda-forge -c pytorch -c defaults \\\n",
    "#   cudf=0.9 cuml=0.9 cugraph=0.9 python=3.6 cudatoolkit=10.0\n",
    "\n",
    "# # set environment vars\n",
    "# import sys, os, shutil\n",
    "# sys.path.append('/usr/local/lib/python3.6/site-packages/')\n",
    "# os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
    "# os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n",
    "\n",
    "# # copy .so files to current working dir\n",
    "# for fn in ['libcudf.so', 'librmm.so']:\n",
    "#     shutil.copy('/usr/local/lib/'+fn, os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BlazingSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install BlazingSQL for CUDA 10.0\n",
    "# ! conda install -q -y --prefix /usr/local -c conda-forge -c defaults -c nvidia -c rapidsai \\\n",
    "#    -c blazingsql/label/cuda10.0 -c blazingsql \\\n",
    "#    blazingsql-calcite blazingsql-orchestrator blazingsql-ral blazingsql-python\n",
    "\n",
    "# !pip install flatbuffers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and create Blazing Context\n",
    "You can think of the BlazingContext much like a Spark Context (i.e. where information such as FileSystems you have registered and Tables you have created will be stored). If you have issues running this cell, restart runtime and try running it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ojm_V-WAtz0f",
    "outputId": "a46625f4-1494-4a13-eb13-2f38efd80ccf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlazingContext ready\n"
     ]
    }
   ],
   "source": [
    "from blazingsql import BlazingContext\n",
    "import cudf\n",
    "\n",
    "bc = BlazingContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yp7z8bfivbna"
   },
   "source": [
    "### Load & Query Table\n",
    "First, we need to download the netflow data (20 million records) from AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "2dAt6DfG37KH",
    "outputId": "b8088e48-8163-4bf1-dcf8-7812293d61e7"
   },
   "outputs": [],
   "source": [
    "# # takes a few minutes to download\n",
    "# !wget https://blazingsql-colab.s3.amazonaws.com/netflow_data/nf-chunk2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BlazingSQL + cuDF \n",
    "Data in hand, we can test the preformance of cuDF and BlazingSQL on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "rirBsYQU3NH5",
    "outputId": "51ced2b1-b930-4173-bbfa-09672e751d3f"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "rmm_allocator::allocate(): RMM_ALLOC: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f6ca0cb35976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %%time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# # Load CSVs into GPU DataFrames (gdf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnetflow_gdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nf-chunk2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/rapidsenv/lib/python3.7/site-packages/cudf/io/csv.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, lineterminator, quotechar, quoting, doublequote, header, mangle_dupe_cols, usecols, sep, delimiter, delim_whitespace, skipinitialspace, names, dtype, skipfooter, skiprows, dayfirst, compression, thousands, decimal, true_values, false_values, nrows, byte_range, skip_blank_lines, parse_dates, comment, na_values, keep_default_na, na_filter, prefix, index_col, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mna_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_filter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     )\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcudf/_lib/csv.pyx\u001b[0m in \u001b[0;36mcudf._lib.csv.read_csv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcudf/_lib/csv.pyx\u001b[0m in \u001b[0;36mcudf._lib.csv.read_csv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: rmm_allocator::allocate(): RMM_ALLOC: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # Load CSVs into GPU DataFrames (gdf)\n",
    "netflow_gdf = cudf.read_csv('nf-chunk2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "zCzLEFfB3N4k",
    "outputId": "10ff9097-2736-423e-969d-de75983fbdda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error on create_table, can only concatenate str (not \"tuple\") to str\n",
      "CPU times: user 9.29 ms, sys: 490 Âµs, total: 9.78 ms\n",
      "Wall time: 96.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyblazing.apiv2.sql.Table at 0x7f66f6c5f8d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Load CSVs into GPU DataFrames (gdf)\n",
    "\n",
    "# Create BlazingSQL Tables - There is no copy in this process\n",
    "bc.create_table('netflow', 'nf-chunk2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "umBG2Tp0wbQx",
    "outputId": "0975395e-7f5b-4244-afa3-45c8658ce61c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communication error: connection to ('localhost', 8889) was lost\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapidsenv/lib/python3.7/site-packages/pyblazing/apiv2/sql.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdask_client\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minternal_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_query_get_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetaToken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartTime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mdask_futures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapidsenv/lib/python3.7/site-packages/pyblazing/apiv2/bridge.py\u001b[0m in \u001b[0;36mrun_query_get_results\u001b[0;34m(client, metaToken, startTime)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_query_get_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetaToken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpyblazing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_query_get_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetaToken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapidsenv/lib/python3.7/site-packages/pyblazing/api.py\u001b[0m in \u001b[0;36mrun_query_get_results\u001b[0;34m(metaToken, startTime)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_query_get_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetaToken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_run_query_get_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetaToken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_run_query_get_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistMetaToken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapidsenv/lib/python3.7/site-packages/pyblazing/api.py\u001b[0m in \u001b[0;36m_run_query_get_results\u001b[0;34m(distMetaToken, startTime)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0mresult_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistMetaToken\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             resultSet, ipchandles = _private_get_result(result.resultToken,\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# make a query\n",
    "sql = '''\n",
    "SELECT\n",
    "  a.firstSeenSrcIp as source,\n",
    "  a.firstSeenDestIp as destination,\n",
    "  count(a.firstSeenDestPort) as targetPorts,\n",
    "  SUM(a.firstSeenSrcTotalBytes) as bytesOut,\n",
    "  SUM(a.firstSeenDestTotalBytes) as bytesIn,\n",
    "  SUM(a.durationSeconds) as durationSeconds,\n",
    "  MIN(parsedDate) as firstFlowDate,\n",
    "  MAX(parsedDate) as lastFlowDate,\n",
    "  COUNT(*) as attemptCount\n",
    "  FROM\n",
    "  main.netflow a\n",
    "  GROUP BY\n",
    "  a.firstSeenSrcIp,\n",
    "  a.firstSeenDestIp\n",
    "  '''\n",
    "\n",
    "# query the table\n",
    "result = bc.sql(sql).get()\n",
    "# set dataframe from results\n",
    "result_gdf = result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_gdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f21fbb5f9c5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# how's it look?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult_gdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result_gdf' is not defined"
     ]
    }
   ],
   "source": [
    "# how's it look?\n",
    "result_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PXbjW1hTxrD"
   },
   "source": [
    "## Apache Spark\n",
    "The cell below installs Apache Spark ([PySpark](https://spark.apache.org/docs/latest/api/python/index.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "pnEEvVEtT8xi",
    "outputId": "1cc4f500-3d98-4df9-fe88-1eb569f35699"
   },
   "outputs": [],
   "source": [
    "# # Note: This installs Spark (version 2.4.1, as tested in April 2019)\n",
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyBlazing vs PySpark\n",
    "With everything installed we can launch a SparkSession and see how BlazingSQL stacks up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "nioEt2MqT9B0",
    "outputId": "f75b9823-5dbd-45b1-9282-562d3d6ddaf0"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#I copied this cell's snippet from another Google Colab by Luca Canali here: https://colab.research.google.com/github/LucaCanali/sparkMeasure/blob/master/examples/SparkMeasure_Jupyter_Colab_Example.ipynb\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "# This example uses a local cluster, you can modify master to use  YARN or K8S if available \n",
    "# This example downloads sparkMeasure 0.13 for scala 2_11 from maven central\n",
    "\n",
    "spark = SparkSession \\\n",
    " .builder \\\n",
    " .master(\"local[*]\") \\\n",
    " .appName(\"PySpark Netflow Benchmark code\") \\\n",
    " .config(\"spark.jars.packages\",\"ch.cern.sparkmeasure:spark-measure_2.11:0.13\")  \\\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G8XSppQiUdLY"
   },
   "source": [
    "### Load & Query Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ZSLuSYSOUDtf",
    "outputId": "2b93169b-63c5-4c46-da14-af87645bf51b"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "netflow_df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('nf-chunk2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "iT3BwLn8UDwE",
    "outputId": "4eeff800-489f-4230-adb9-f3a1c16ede66"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "netflow_df.createOrReplaceTempView('netflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "9SBhahA5UD2k",
    "outputId": "accc1938-6470-44df-ab7f-70058c755b2b"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sql = '''\n",
    "SELECT\n",
    "  a.firstSeenSrcIp as source,\n",
    "  a.firstSeenDestIp as destination,\n",
    "  count(a.firstSeenDestPort) as targetPorts,\n",
    "  SUM(a.firstSeenSrcTotalBytes) as bytesOut,\n",
    "  SUM(a.firstSeenDestTotalBytes) as bytesIn,\n",
    "  SUM(a.durationSeconds) as durationSeconds,\n",
    "  MIN(parsedDate) as firstFlowDate,\n",
    "  MAX(parsedDate) as lastFlowDate,\n",
    "  COUNT(*) as attemptCount\n",
    "  FROM\n",
    "  netflow a\n",
    "  GROUP BY\n",
    "  a.firstSeenSrcIp,\n",
    "  a.firstSeenDestIp\n",
    "  '''\n",
    "\n",
    "# dataframe results\n",
    "edges_df = spark.sql(sql)\n",
    "\n",
    "# display results\n",
    "edges_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BlazingSQL_vs_PySpark_Netflow.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
