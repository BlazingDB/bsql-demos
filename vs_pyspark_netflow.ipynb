{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d0hJ4z8rBOFC"
   },
   "source": [
    "# BlazingSQL vs. Apache Spark \n",
    "\n",
    "Below we have one of our popular workloads running with [BlazingSQL + RAPIDS AI](https://blazingdb.com) and then running the entire ETL phase again, only this time with Apache Spark + PySpark.\n",
    "\n",
    "In this notebook, we will cover: \n",
    "- How to set up [BlazingSQL](https://blazingsql.com) and the [RAPIDS AI](https://rapids.ai/) suite.\n",
    "- How to read and query csv files with cuDF and BlazingSQL.\n",
    "- How BlazingSQL compares against Apache Spark (analyzing over 20M records).\n",
    "\n",
    "![Impression](https://www.google-analytics.com/collect?v=1&tid=UA-39814657-5&cid=555&t=event&ec=guides&ea=bsql_vs_spark&dt=bsql_vs_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJyD4oSbugE0"
   },
   "source": [
    "## Setup\n",
    "### Environment Sanity Check \n",
    "\n",
    "RAPIDS packages (BlazingSQL included) require Pascal+ architecture to run. For Colab, this translates to a T4 GPU instance. \n",
    "\n",
    "The cell below will let you know what type of GPU you've been allocated, and how to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "QzVzojZ7tc9a",
    "outputId": "1c412c49-59fd-482b-83dc-1764af8fda12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woo! You got the right kind of GPU!\n"
     ]
    }
   ],
   "source": [
    "# tag specs\n",
    "colab_smi = !nvidia-smi\n",
    "\n",
    "# focus GPU type\n",
    "try:\n",
    "    my_gpu = ' '.join(colab_smi[7].split()[2:4])\n",
    "# not on gpu acceleration \n",
    "except:\n",
    "    raise Exception(\"\\nPlease make sure you've configured Colab to request a GPU instance type.\\n\\n\"\n",
    "                    \"At top of Colab, try: Runtime -> Change runtime type -> Hardware accelerator -> GPU -> Save\\n\")\n",
    "\n",
    "# not allocated compatable GPU\n",
    "if (my_gpu != b'Tesla T4') and (my_gpu != 'Tesla P100-PCIE...') and (my_gpu != 'GeForce GTX'):\n",
    "    # allocated K80\n",
    "    if my_gpu == 'Tesla K80':\n",
    "        raise Exception(\"\\nYou've been allocated a K80 instance\\n\\n\"\n",
    "                    \"Unfortunately, this demo requires a T4 instance\\n\\n\"\n",
    "                    \"At top of Colab, try: Runtime -> Reset all runtimes...\\n\")\n",
    "    else:\n",
    "        raise Exception(f\"\\nYou've achieved wizardy.\\nyour GPU is {my_gpu}\\nPlease inform info@blazingsql.com\")\n",
    "\n",
    "# allocated compatable GPU\n",
    "else:\n",
    "    print('Woo! You got the right kind of GPU!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "btG1BbSA1nLu",
    "outputId": "2bce70f9-8d92-4a16-baa2-2b190d785142"
   },
   "source": [
    "## Installs \n",
    "\n",
    "Below you will find three code blocks:\n",
    "1. The first installs miniconda.\n",
    "2. The second installs RAPIDS AI and sets up the system environment. \n",
    "3. The third installs BlazingSQL.\n",
    "\n",
    "### Miniconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqQ8lqL8vb-8"
   },
   "outputs": [],
   "source": [
    "# intall miniconda\n",
    "!wget -c https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
    "!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n",
    "!bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAPIDS AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install RAPIDS packages\n",
    "!conda install -q -y --prefix /usr/local -c nvidia -c rapidsai \\\n",
    "  -c numba -c conda-forge -c pytorch -c defaults \\\n",
    "  cudf=0.9 cuml=0.9 cugraph=0.9 python=3.6 cudatoolkit=10.0\n",
    "\n",
    "# set environment vars\n",
    "import sys, os, shutil\n",
    "sys.path.append('/usr/local/lib/python3.6/site-packages/')\n",
    "os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
    "os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n",
    "\n",
    "# copy .so files to current working dir\n",
    "for fn in ['libcudf.so', 'librmm.so']:\n",
    "    shutil.copy('/usr/local/lib/'+fn, os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BlazingSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install BlazingSQL for CUDA 10.0\n",
    "! conda install -q -y --prefix /usr/local -c conda-forge -c defaults -c nvidia -c rapidsai \\\n",
    "   -c blazingsql/label/cuda10.0 -c blazingsql \\\n",
    "   blazingsql-calcite blazingsql-orchestrator blazingsql-ral blazingsql-python\n",
    "\n",
    "!pip install flatbuffers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and create Blazing Context\n",
    "You can think of the BlazingContext much like a Spark Context (i.e. where information such as FileSystems you have registered and Tables you have created will be stored). If you have issues running this cell, restart runtime and try running it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ojm_V-WAtz0f",
    "outputId": "a46625f4-1494-4a13-eb13-2f38efd80ccf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection established\n"
     ]
    }
   ],
   "source": [
    "from blazingsql import BlazingContext\n",
    "import cudf\n",
    "\n",
    "bc = BlazingContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yp7z8bfivbna"
   },
   "source": [
    "### Load & Query Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "2dAt6DfG37KH",
    "outputId": "b8088e48-8163-4bf1-dcf8-7812293d61e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-23 23:08:49--  https://blazingsql-colab.s3.amazonaws.com/netflow_data/nf-chunk2.csv\n",
      "Resolving blazingsql-colab.s3.amazonaws.com (blazingsql-colab.s3.amazonaws.com)... 52.216.144.139\n",
      "Connecting to blazingsql-colab.s3.amazonaws.com (blazingsql-colab.s3.amazonaws.com)|52.216.144.139|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2725056295 (2.5G) [text/csv]\n",
      "Saving to: ‘nf-chunk2.csv’\n",
      "\n",
      "nf-chunk2.csv       100%[===================>]   2.54G  46.2MB/s    in 54s     \n",
      "\n",
      "2019-08-23 23:09:43 (48.2 MB/s) - ‘nf-chunk2.csv’ saved [2725056295/2725056295]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://blazingsql-colab.s3.amazonaws.com/netflow_data/nf-chunk2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "rirBsYQU3NH5",
    "outputId": "51ced2b1-b930-4173-bbfa-09672e751d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.18 s, sys: 1.49 s, total: 5.67 s\n",
      "Wall time: 5.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Load CSVs into GPU DataFrames (gdf)\n",
    "netflow_gdf = cudf.read_csv('nf-chunk2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "zCzLEFfB3N4k",
    "outputId": "10ff9097-2736-423e-969d-de75983fbdda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.6 ms, sys: 136 µs, total: 28.8 ms\n",
      "Wall time: 92.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Create BlazingSQL Tables - There is no copy in this process\n",
    "bc.create_table('netflow', netflow_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "umBG2Tp0wbQx",
    "outputId": "0975395e-7f5b-4244-afa3-45c8658ce61c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: You no longer need to send a table list to the .sql() funtion\n",
      "  firstSeenSrcIp firstSeenDestIp  ...         lastFlowDate  attemptCount\n",
      "0   172.10.1.234        10.0.0.5  ...  2013-04-03 15:11:07           104\n",
      "1      10.1.0.76     172.10.1.82  ...  2013-04-03 09:55:05             1\n",
      "2    172.30.1.85        10.0.0.8  ...  2013-04-03 12:06:53            84\n",
      "3    172.30.2.60        10.0.0.9  ...  2013-04-03 12:12:37            82\n",
      "4   172.10.1.106    10.199.250.2  ...  2013-04-03 10:12:35            40\n",
      "5       10.0.0.9    172.30.1.124  ...  2013-04-03 10:36:04             1\n",
      "6   172.30.2.125        10.0.0.9  ...  2013-04-03 12:12:37            69\n",
      "7    172.30.1.10       10.0.0.12  ...  2013-04-03 12:11:40            69\n",
      "8    172.20.1.58        10.7.5.5  ...  2013-04-03 11:20:09            49\n",
      "9    172.20.1.53        10.0.0.7  ...  2013-04-03 11:22:04            67\n",
      "\n",
      "[10 rows x 9 columns]\n",
      "CPU times: user 62.6 ms, sys: 10.9 ms, total: 73.5 ms\n",
      "Wall time: 2.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sql = '''\n",
    "SELECT\n",
    "  a.firstSeenSrcIp as source,\n",
    "  a.firstSeenDestIp as destination,\n",
    "  count(a.firstSeenDestPort) as targetPorts,\n",
    "  SUM(a.firstSeenSrcTotalBytes) as bytesOut,\n",
    "  SUM(a.firstSeenDestTotalBytes) as bytesIn,\n",
    "  SUM(a.durationSeconds) as durationSeconds,\n",
    "  MIN(parsedDate) as firstFlowDate,\n",
    "  MAX(parsedDate) as lastFlowDate,\n",
    "  COUNT(*) as attemptCount\n",
    "  FROM\n",
    "  main.netflow a\n",
    "  GROUP BY\n",
    "  a.firstSeenSrcIp,\n",
    "  a.firstSeenDestIp\n",
    "  '''\n",
    "\n",
    "result = bc.sql(sql,['netflow']).get()\n",
    "\n",
    "# print(result.columns)\n",
    "\n",
    "result_gdf = result.columns\n",
    "edges_df = result_gdf.to_pandas()\n",
    "print(edges_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PXbjW1hTxrD"
   },
   "source": [
    "## Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "pnEEvVEtT8xi",
    "outputId": "1cc4f500-3d98-4df9-fe88-1eb569f35699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/98/244399c0daa7894cdf387e7007d5e8b3710a79b67f3fd991c0b0b644822d/pyspark-2.4.3.tar.gz (215.6MB)\n",
      "\u001b[K     |████████████████████████████████| 215.6MB 125kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 44.6MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-2.4.3-py2.py3-none-any.whl size=215964963 sha256=309c5916fd287e31dd2c8d7fd305111b567fc1f6893d5d2a51e153d98c161fba\n",
      "  Stored in directory: /root/.cache/pip/wheels/8d/20/f0/b30e2024226dc112e256930dd2cd4f06d00ab053c86278dcf3\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.3\n",
      "CPU times: user 1.07 s, sys: 2.32 s, total: 3.39 s\n",
      "Wall time: 36.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Install Spark \n",
    "# Note: This installs Spark (version 2.4.1, as tested in April 2019)\n",
    "\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "nioEt2MqT9B0",
    "outputId": "f75b9823-5dbd-45b1-9282-562d3d6ddaf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.6 ms, sys: 31.3 ms, total: 86 ms\n",
      "Wall time: 9.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#I copied this cell's snippet from another Google Colab by Luca Canali here: https://colab.research.google.com/github/LucaCanali/sparkMeasure/blob/master/examples/SparkMeasure_Jupyter_Colab_Example.ipynb\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "# This example uses a local cluster, you can modify master to use  YARN or K8S if available \n",
    "# This example downloads sparkMeasure 0.13 for scala 2_11 from maven central\n",
    "\n",
    "spark = SparkSession \\\n",
    " .builder \\\n",
    " .master(\"local[*]\") \\\n",
    " .appName(\"PySpark Netflow Benchmark code\") \\\n",
    " .config(\"spark.jars.packages\",\"ch.cern.sparkmeasure:spark-measure_2.11:0.13\")  \\\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G8XSppQiUdLY"
   },
   "source": [
    "###Load & Query Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ZSLuSYSOUDtf",
    "outputId": "2b93169b-63c5-4c46-da14-af87645bf51b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 121 ms, sys: 81.4 ms, total: 202 ms\n",
      "Wall time: 5min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "netflow_df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('nf-chunk2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "iT3BwLn8UDwE",
    "outputId": "4eeff800-489f-4230-adb9-f3a1c16ede66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 822 µs, sys: 1.43 ms, total: 2.25 ms\n",
      "Wall time: 200 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "netflow_df.createOrReplaceTempView('netflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "9SBhahA5UD2k",
    "outputId": "accc1938-6470-44df-ab7f-70058c755b2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+-----------+--------+-------+---------------+-------------------+-------------------+------------+\n",
      "|      source|    destination|targetPorts|bytesOut|bytesIn|durationSeconds|      firstFlowDate|       lastFlowDate|attemptCount|\n",
      "+------------+---------------+-----------+--------+-------+---------------+-------------------+-------------------+------------+\n",
      "| 172.10.1.13|239.255.255.250|         15|    2975|      0|              6|2013-04-03 06:36:19|2013-04-03 06:36:27|          15|\n",
      "|172.30.1.204|239.255.255.250|          8|    1750|      0|              6|2013-04-03 06:36:13|2013-04-03 06:36:20|           8|\n",
      "| 172.30.2.86|      172.0.0.1|          1|     540|      0|              2|2013-04-03 06:36:09|2013-04-03 06:36:09|           1|\n",
      "|172.30.1.246|      172.0.0.1|         29|    2610|   2610|              0|2013-04-03 00:26:46|2013-04-03 23:06:00|          29|\n",
      "| 172.30.1.51|239.255.255.250|         16|    3850|      0|             18|2013-04-03 06:35:22|2013-04-03 06:44:08|          16|\n",
      "| 172.10.1.35|      172.0.0.1|          1|     270|      0|              0|2013-04-03 06:36:21|2013-04-03 06:36:21|           1|\n",
      "| 172.20.1.91|239.255.255.250|         19|    3675|      0|              6|2013-04-03 06:36:50|2013-04-03 06:36:59|          19|\n",
      "|172.20.1.249|239.255.255.250|          2|     700|      0|              6|2013-04-03 06:37:17|2013-04-03 06:37:23|           2|\n",
      "|172.10.1.232|      172.0.0.1|         30|    3060|   3060|             48|2013-04-03 01:31:31|2013-04-03 22:53:36|          30|\n",
      "|172.10.1.238|239.255.255.250|          2|     700|      0|              6|2013-04-03 06:36:44|2013-04-03 06:36:51|           2|\n",
      "|172.10.2.137|      172.0.0.1|         28|    2520|   2250|              0|2013-04-03 01:10:48|2013-04-03 23:15:00|          28|\n",
      "|172.30.1.131|       10.0.0.5|         83|   37545|  52348|              2|2013-04-03 06:48:00|2013-04-03 12:01:20|          83|\n",
      "|172.30.1.200|       10.0.0.6|         70|   31548|  44024|              2|2013-04-03 06:48:03|2013-04-03 12:02:30|          70|\n",
      "|172.30.1.115|      10.0.0.14|         71|   32126|  44426|              2|2013-04-03 06:48:05|2013-04-03 11:49:25|          71|\n",
      "|172.30.2.145|       10.0.0.8|         61|   26564|  37118|              5|2013-04-03 06:48:09|2013-04-03 12:12:36|          61|\n",
      "|172.30.1.112|       10.0.0.5|         72|   32562|  45450|             19|2013-04-03 06:48:09|2013-04-03 11:44:06|          72|\n",
      "|172.30.1.239|      10.0.0.11|        120|   53980|  74810|              0|2013-04-03 06:49:20|2013-04-03 14:51:26|         120|\n",
      "|172.20.1.246|      10.0.0.13|        111|   50348|  68303|             58|2013-04-03 06:49:22|2013-04-03 15:03:48|         111|\n",
      "|172.30.2.146|      10.0.0.10|         68|   30480|  42357|              7|2013-04-03 06:48:46|2013-04-03 12:06:50|          68|\n",
      "|172.10.1.151|      10.1.0.76|         64|   29414|  40458|             17|2013-04-03 06:48:37|2013-04-03 14:59:43|          64|\n",
      "+------------+---------------+-----------+--------+-------+---------------+-------------------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 55.7 ms, sys: 38 ms, total: 93.6 ms\n",
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sql = '''\n",
    "SELECT\n",
    "  a.firstSeenSrcIp as source,\n",
    "  a.firstSeenDestIp as destination,\n",
    "  count(a.firstSeenDestPort) as targetPorts,\n",
    "  SUM(a.firstSeenSrcTotalBytes) as bytesOut,\n",
    "  SUM(a.firstSeenDestTotalBytes) as bytesIn,\n",
    "  SUM(a.durationSeconds) as durationSeconds,\n",
    "  MIN(parsedDate) as firstFlowDate,\n",
    "  MAX(parsedDate) as lastFlowDate,\n",
    "  COUNT(*) as attemptCount\n",
    "  FROM\n",
    "  netflow a\n",
    "  GROUP BY\n",
    "  a.firstSeenSrcIp,\n",
    "  a.firstSeenDestIp\n",
    "  '''\n",
    "\n",
    "edges_df = spark.sql(sql)\n",
    "\n",
    "edges_df.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BlazingSQL_vs_PySpark_Netflow.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
