{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d0hJ4z8rBOFC"
   },
   "source": [
    "# BlazingSQL vs. Apache Spark \n",
    "\n",
    "Below we have one of our popular workloads running with [BlazingSQL + RAPIDS AI](https://blazingdb.com) and then running the entire ETL phase again, only this time with Apache Spark + PySpark.\n",
    "\n",
    "In this notebook, we will cover: \n",
    "- How to set up [BlazingSQL](https://blazingsql.com) and the [RAPIDS AI](https://rapids.ai/) suite.\n",
    "- How to read and query csv files with cuDF and BlazingSQL.\n",
    "- How BlazingSQL compares against Apache Spark (analyzing over 20M records).\n",
    "\n",
    "![Impression](https://www.google-analytics.com/collect?v=1&tid=UA-39814657-5&cid=555&t=event&ec=guides&ea=bsql_vs_spark&dt=bsql_vs_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJyD4oSbugE0"
   },
   "source": [
    "## Setup\n",
    "### Environment Sanity Check \n",
    "\n",
    "RAPIDS packages (BlazingSQL included) require Pascal+ architecture to run. For Colab, this translates to a T4 GPU instance. \n",
    "\n",
    "The cell below will let you know what type of GPU you've been allocated, and how to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "QzVzojZ7tc9a",
    "outputId": "1c412c49-59fd-482b-83dc-1764af8fda12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woo! You got the right kind of GPU!\n"
     ]
    }
   ],
   "source": [
    "# tag specs\n",
    "colab_smi = !nvidia-smi\n",
    "\n",
    "# focus GPU type\n",
    "try:\n",
    "    my_gpu = ' '.join(colab_smi[7].split()[2:4])\n",
    "# not on gpu acceleration \n",
    "except:\n",
    "    raise Exception(\"\\nPlease make sure you've configured Colab to request a GPU instance type.\\n\\n\"\n",
    "                    \"At top of Colab, try: Runtime -> Change runtime type -> Hardware accelerator -> GPU -> Save\\n\")\n",
    "\n",
    "# not allocated compatable GPU\n",
    "if (my_gpu != b'Tesla T4') and (my_gpu != 'Tesla P100-PCIE...') and (my_gpu != 'GeForce GTX'):\n",
    "    # allocated K80\n",
    "    if my_gpu == 'Tesla K80':\n",
    "        raise Exception(\"\\nYou've been allocated a K80 instance\\n\\n\"\n",
    "                    \"Unfortunately, this demo requires a T4 instance\\n\\n\"\n",
    "                    \"At top of Colab, try: Runtime -> Reset all runtimes...\\n\")\n",
    "    else:\n",
    "        raise Exception(f\"\\nYou've achieved wizardy.\\nyour GPU is {my_gpu}\\nPlease inform info@blazingsql.com\")\n",
    "\n",
    "# allocated compatable GPU\n",
    "else:\n",
    "    print('Woo! You got the right kind of GPU!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "btG1BbSA1nLu",
    "outputId": "2bce70f9-8d92-4a16-baa2-2b190d785142"
   },
   "source": [
    "## Installs \n",
    "\n",
    "Below you will find three code blocks:\n",
    "1. The first installs miniconda.\n",
    "2. The second installs RAPIDS AI and sets up the system environment. \n",
    "3. The third installs BlazingSQL.\n",
    "\n",
    "### Miniconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqQ8lqL8vb-8"
   },
   "outputs": [],
   "source": [
    "# intall miniconda\n",
    "!wget -c https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
    "!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n",
    "!bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAPIDS AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install RAPIDS packages\n",
    "!conda install -q -y --prefix /usr/local -c nvidia -c rapidsai \\\n",
    "  -c numba -c conda-forge -c pytorch -c defaults \\\n",
    "  cudf=0.9 cuml=0.9 cugraph=0.9 python=3.6 cudatoolkit=10.0\n",
    "\n",
    "# set environment vars\n",
    "import sys, os, shutil\n",
    "sys.path.append('/usr/local/lib/python3.6/site-packages/')\n",
    "os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
    "os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n",
    "\n",
    "# copy .so files to current working dir\n",
    "for fn in ['libcudf.so', 'librmm.so']:\n",
    "    shutil.copy('/usr/local/lib/'+fn, os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BlazingSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install BlazingSQL for CUDA 10.0\n",
    "! conda install -q -y --prefix /usr/local -c conda-forge -c defaults -c nvidia -c rapidsai \\\n",
    "   -c blazingsql/label/cuda10.0 -c blazingsql \\\n",
    "   blazingsql-calcite blazingsql-orchestrator blazingsql-ral blazingsql-python\n",
    "\n",
    "!pip install flatbuffers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and create Blazing Context\n",
    "You can think of the BlazingContext much like a Spark Context (i.e. where information such as FileSystems you have registered and Tables you have created will be stored). If you have issues running this cell, restart runtime and try running it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ojm_V-WAtz0f",
    "outputId": "a46625f4-1494-4a13-eb13-2f38efd80ccf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlazingContext ready\n"
     ]
    }
   ],
   "source": [
    "from blazingsql import BlazingContext\n",
    "import cudf\n",
    "\n",
    "bc = BlazingContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yp7z8bfivbna"
   },
   "source": [
    "### Load & Query Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "2dAt6DfG37KH",
    "outputId": "b8088e48-8163-4bf1-dcf8-7812293d61e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-22 02:31:46--  https://blazingsql-colab.s3.amazonaws.com/netflow_data/nf-chunk2.csv\n",
      "Resolving blazingsql-colab.s3.amazonaws.com (blazingsql-colab.s3.amazonaws.com)... 52.216.66.0\n",
      "Connecting to blazingsql-colab.s3.amazonaws.com (blazingsql-colab.s3.amazonaws.com)|52.216.66.0|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2725056295 (2.5G) [text/csv]\n",
      "Saving to: ‘nf-chunk2.csv.1’\n",
      "\n",
      "nf-chunk2.csv.1     100%[===================>]   2.54G  18.8MB/s    in 2m 20s  \n",
      "\n",
      "2019-10-22 02:34:06 (18.6 MB/s) - ‘nf-chunk2.csv.1’ saved [2725056295/2725056295]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# takes a few minutes to download\n",
    "!wget https://blazingsql-colab.s3.amazonaws.com/netflow_data/nf-chunk2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "rirBsYQU3NH5",
    "outputId": "51ced2b1-b930-4173-bbfa-09672e751d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 148 ms, sys: 181 ms, total: 329 ms\n",
      "Wall time: 339 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Load CSVs into GPU DataFrames (gdf)\n",
    "netflow_gdf = cudf.read_csv('nf-chunk2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "zCzLEFfB3N4k",
    "outputId": "10ff9097-2736-423e-969d-de75983fbdda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.6 ms, sys: 0 ns, total: 28.6 ms\n",
      "Wall time: 66.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyblazing.apiv2.sql.Table at 0x7fe618998160>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Create BlazingSQL Tables - There is no copy in this process\n",
    "bc.create_table('netflow', netflow_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "umBG2Tp0wbQx",
    "outputId": "0975395e-7f5b-4244-afa3-45c8658ce61c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: You no longer need to send a table list to the .sql() funtion\n",
      "          source      destination  targetPorts  bytesOut  bytesIn  \\\n",
      "0     172.10.0.6     172.10.1.251            1        74        0   \n",
      "1    172.20.2.75  239.255.255.250            4      1050        0   \n",
      "2   172.30.2.127        10.0.0.14            1       454      633   \n",
      "3   172.30.1.160  239.255.255.250           22      4550        0   \n",
      "4    172.30.2.68        172.0.0.1            1       270        0   \n",
      "5    172.30.1.27        10.0.0.13            1       454      633   \n",
      "6    172.30.1.11        172.0.0.1            2       450      450   \n",
      "7    172.30.1.49  239.255.255.250           16      3850        0   \n",
      "8   172.20.1.244  239.255.255.250            2       700        0   \n",
      "9  10.170.32.181       172.20.0.4           10    132500  4609780   \n",
      "\n",
      "   durationSeconds        firstFlowDate         lastFlowDate  attemptCount  \n",
      "0                0  2013-04-03 06:47:57  2013-04-03 06:47:57             1  \n",
      "1                6  2013-04-03 06:37:11  2013-04-03 06:37:18             4  \n",
      "2                0  2013-04-03 06:48:05  2013-04-03 06:48:05             1  \n",
      "3               12  2013-04-03 06:35:51  2013-04-03 06:43:26            22  \n",
      "4                1  2013-04-03 06:36:09  2013-04-03 06:36:09             1  \n",
      "5                0  2013-04-03 06:48:07  2013-04-03 06:48:07             1  \n",
      "6               65  2013-04-03 06:35:23  2013-04-03 06:44:02             2  \n",
      "7               18  2013-04-03 06:35:22  2013-04-03 06:44:08            16  \n",
      "8                6  2013-04-03 06:37:17  2013-04-03 06:37:23             2  \n",
      "9               30  2013-04-03 00:12:31  2013-04-03 00:15:39            10  \n",
      "CPU times: user 52.2 ms, sys: 3.17 ms, total: 55.4 ms\n",
      "Wall time: 463 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sql = '''\n",
    "SELECT\n",
    "  a.firstSeenSrcIp as source,\n",
    "  a.firstSeenDestIp as destination,\n",
    "  count(a.firstSeenDestPort) as targetPorts,\n",
    "  SUM(a.firstSeenSrcTotalBytes) as bytesOut,\n",
    "  SUM(a.firstSeenDestTotalBytes) as bytesIn,\n",
    "  SUM(a.durationSeconds) as durationSeconds,\n",
    "  MIN(parsedDate) as firstFlowDate,\n",
    "  MAX(parsedDate) as lastFlowDate,\n",
    "  COUNT(*) as attemptCount\n",
    "  FROM\n",
    "  main.netflow a\n",
    "  GROUP BY\n",
    "  a.firstSeenSrcIp,\n",
    "  a.firstSeenDestIp\n",
    "  '''\n",
    "\n",
    "result = bc.sql(sql,['netflow']).get()\n",
    "\n",
    "# print(result.columns)\n",
    "\n",
    "result_gdf = result.columns\n",
    "edges_df = result_gdf.to_pandas()\n",
    "print(edges_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PXbjW1hTxrD"
   },
   "source": [
    "## Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "pnEEvVEtT8xi",
    "outputId": "1cc4f500-3d98-4df9-fe88-1eb569f35699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
      "\u001b[K     |████████████████████████████████| 215.7MB 39.2MB/s eta 0:00:01   |████▏                           | 28.2MB 1.7MB/s eta 0:01:52\n",
      "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 25.4MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130387 sha256=184d337b621c50c97cd08c1e1463a04c26f37bec88f54f221030a1c9b78e12c7\n",
      "  Stored in directory: /home/rodrigo/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.4\n",
      "CPU times: user 1.54 s, sys: 510 ms, total: 2.05 s\n",
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Install Spark \n",
    "# Note: This installs Spark (version 2.4.1, as tested in April 2019)\n",
    "\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "nioEt2MqT9B0",
    "outputId": "f75b9823-5dbd-45b1-9282-562d3d6ddaf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.5 ms, sys: 15.5 ms, total: 63 ms\n",
      "Wall time: 6.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#I copied this cell's snippet from another Google Colab by Luca Canali here: https://colab.research.google.com/github/LucaCanali/sparkMeasure/blob/master/examples/SparkMeasure_Jupyter_Colab_Example.ipynb\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "# This example uses a local cluster, you can modify master to use  YARN or K8S if available \n",
    "# This example downloads sparkMeasure 0.13 for scala 2_11 from maven central\n",
    "\n",
    "spark = SparkSession \\\n",
    " .builder \\\n",
    " .master(\"local[*]\") \\\n",
    " .appName(\"PySpark Netflow Benchmark code\") \\\n",
    " .config(\"spark.jars.packages\",\"ch.cern.sparkmeasure:spark-measure_2.11:0.13\")  \\\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G8XSppQiUdLY"
   },
   "source": [
    "### Load & Query Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ZSLuSYSOUDtf",
    "outputId": "2b93169b-63c5-4c46-da14-af87645bf51b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.58 ms, sys: 772 µs, total: 3.35 ms\n",
      "Wall time: 2.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "netflow_df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('nf-chunk2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "iT3BwLn8UDwE",
    "outputId": "4eeff800-489f-4230-adb9-f3a1c16ede66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.35 ms, sys: 405 µs, total: 1.76 ms\n",
      "Wall time: 151 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "netflow_df.createOrReplaceTempView('netflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "9SBhahA5UD2k",
    "outputId": "accc1938-6470-44df-ab7f-70058c755b2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-----------+--------+-------+---------------+-------------------+-------------------+------------+\n",
      "|       source|    destination|targetPorts|bytesOut|bytesIn|durationSeconds|      firstFlowDate|       lastFlowDate|attemptCount|\n",
      "+-------------+---------------+-----------+--------+-------+---------------+-------------------+-------------------+------------+\n",
      "|  172.10.1.13|239.255.255.250|         15|    2975|      0|              6|2013-04-03 06:36:19|2013-04-03 06:36:27|          15|\n",
      "| 172.10.1.232|      172.0.0.1|          1|     180|    180|              0|2013-04-03 06:36:45|2013-04-03 06:36:45|           1|\n",
      "| 172.10.1.238|239.255.255.250|          2|     700|      0|              6|2013-04-03 06:36:44|2013-04-03 06:36:51|           2|\n",
      "|  172.10.1.35|      172.0.0.1|          1|     270|      0|              0|2013-04-03 06:36:21|2013-04-03 06:36:21|           1|\n",
      "| 172.10.2.137|      172.0.0.1|          1|      90|     90|              0|2013-04-03 06:36:42|2013-04-03 06:36:42|           1|\n",
      "| 172.20.1.249|239.255.255.250|          2|     700|      0|              6|2013-04-03 06:37:17|2013-04-03 06:37:23|           2|\n",
      "|  172.20.1.91|239.255.255.250|         19|    3675|      0|              6|2013-04-03 06:36:50|2013-04-03 06:36:59|          19|\n",
      "| 172.30.1.112|       10.0.0.5|          1|     453|    632|              0|2013-04-03 06:48:09|2013-04-03 06:48:09|           1|\n",
      "| 172.30.1.115|      10.0.0.14|          1|     454|    633|              0|2013-04-03 06:48:05|2013-04-03 06:48:05|           1|\n",
      "| 172.30.1.131|       10.0.0.5|          1|     453|    632|              0|2013-04-03 06:48:00|2013-04-03 06:48:00|           1|\n",
      "| 172.30.1.200|       10.0.0.6|          1|     453|    632|              0|2013-04-03 06:48:03|2013-04-03 06:48:03|           1|\n",
      "| 172.30.1.204|239.255.255.250|          8|    1750|      0|              6|2013-04-03 06:36:13|2013-04-03 06:36:20|           8|\n",
      "| 172.30.1.246|      172.0.0.1|          1|      90|     90|              0|2013-04-03 06:36:14|2013-04-03 06:36:14|           1|\n",
      "|  172.30.1.51|239.255.255.250|         11|    2975|      0|             18|2013-04-03 06:35:22|2013-04-03 06:44:08|          11|\n",
      "| 172.30.2.145|       10.0.0.8|          1|     453|    632|              0|2013-04-03 06:48:09|2013-04-03 06:48:09|           1|\n",
      "|  172.30.2.86|      172.0.0.1|          1|     540|      0|              2|2013-04-03 06:36:09|2013-04-03 06:36:09|           1|\n",
      "|10.247.106.27|     172.20.0.4|          2|   15290| 341090|              6|2013-04-03 06:10:52|2013-04-03 06:10:54|           2|\n",
      "|  172.10.1.55|239.255.255.250|          4|    1050|      0|              6|2013-04-03 06:36:21|2013-04-03 06:36:27|           4|\n",
      "| 172.20.1.202|      172.0.0.1|          1|     270|    270|             18|2013-04-03 06:37:35|2013-04-03 06:37:35|           1|\n",
      "|  172.20.1.50|      172.0.0.1|          1|     360|    360|             18|2013-04-03 06:37:11|2013-04-03 06:37:11|           1|\n",
      "+-------------+---------------+-----------+--------+-------+---------------+-------------------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 2.78 ms, sys: 397 µs, total: 3.17 ms\n",
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sql = '''\n",
    "SELECT\n",
    "  a.firstSeenSrcIp as source,\n",
    "  a.firstSeenDestIp as destination,\n",
    "  count(a.firstSeenDestPort) as targetPorts,\n",
    "  SUM(a.firstSeenSrcTotalBytes) as bytesOut,\n",
    "  SUM(a.firstSeenDestTotalBytes) as bytesIn,\n",
    "  SUM(a.durationSeconds) as durationSeconds,\n",
    "  MIN(parsedDate) as firstFlowDate,\n",
    "  MAX(parsedDate) as lastFlowDate,\n",
    "  COUNT(*) as attemptCount\n",
    "  FROM\n",
    "  netflow a\n",
    "  GROUP BY\n",
    "  a.firstSeenSrcIp,\n",
    "  a.firstSeenDestIp\n",
    "  '''\n",
    "\n",
    "edges_df = spark.sql(sql)\n",
    "\n",
    "edges_df.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BlazingSQL_vs_PySpark_Netflow.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
